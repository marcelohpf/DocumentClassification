@inproceedings{lime,
  author    = {Marco Tulio Ribeiro and
               Sameer Singh and
               Carlos Guestrin},
  title     = {"Why Should {I} Trust You?": Explaining the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on
               Knowledge Discovery and Data Mining, San Francisco, CA, USA, August
               13-17, 2016},
  pages     = {1135--1144},
  year      = {2016},
}
@inproceedings{marco_tulio__ribeiro_anchors:_2018,
	title = {Anchors: {High}-{Precision} {Model}-{Agnostic} {Explanations}},
	abstract = {We introduce a novel model-agnostic system that explains the
behavior of complex models with high-precision rules called
anchors
, representing local, “sufficient” conditions for predic-
tions. We propose an algorithm to efficiently compute these
explanations for any black-box model with high-probability
guarantees. We demonstrate the flexibility of anchors by ex-
plaining a myriad of different models for different domains
and tasks. In a user study, we show that anchors enable users
to predict how a model would behave on unseen instances
with less effort and higher precision, as compared to existing
linear explanations or no explanations.},
	booktitle = {The {Thirty}-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Marco Tulio  Ribeiro and Sameer Singh and Carlos Guestrin},
	month = apr,
	year = {2018},
	pages = {1527--1535}
}
@incollection{NIPS2017_7062,
title = {A Unified Approach to Interpreting Model Predictions},
author = {Lundberg, Scott M and Lee, Su-In},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {4765--4774},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}
}

@article{chen_xgboost:_2016,
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	shorttitle = {{XGBoost}},
	url = {http://arxiv.org/abs/1603.02754},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly eﬀective and widely used machine learning method. In this paper, we describe a scalable endto-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	language = {en},
	urldate = {2018-09-04},
	journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '16},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	note = {arXiv: 1603.02754},
	keywords = {Computer Science - Machine Learning},
	pages = {785--794}
}
@inproceedings{zhou_deep_2017,
	address = {Melbourne, Australia},
	title = {Deep {Forest}: {Towards} {An} {Alternative} to {Deep} {Neural} {Networks}},
	isbn = {978-0-9992411-0-3},
	shorttitle = {Deep {Forest}},
	url = {https://www.ijcai.org/proceedings/2017/497},
	doi = {10.24963/ijcai.2017/497},
	abstract = {In this paper, we propose gcForest, a decision tree ensemble approach with performance highly competitive to deep neural networks in a broad range of tasks. In contrast to deep neural networks which require great effort in hyper-parameter tuning, gcForest is much easier to train; even when it is applied to different data across different domains in our experiments, excellent performance can be achieved by almost same settings of hyper-parameters. The training process of gcForest is efﬁcient, and users can control training cost according to computational resource available. The efﬁciency may be further enhanced because gcForest is naturally apt to parallel implementation. Furthermore, in contrast to deep neural networks which require largescale training data, gcForest can work well even when there are only small-scale training data.},
	language = {en},
	urldate = {2018-11-08},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Zhou, Zhi-Hua and Feng, Ji},
	month = aug,
	year = {2017},
	pages = {3553--3559}
}
@article{gao_sd-cnn:_2018,
	title = {{SD}-{CNN}: a {Shallow}-{Deep} {CNN} for {Improved} {Breast} {Cancer} {Diagnosis}},
	shorttitle = {{SD}-{CNN}},
	url = {http://arxiv.org/abs/1803.00663},
	abstract = {Breast cancer is the second leading cause of cancer death among women worldwide. Nevertheless, it is also one of the most treatable malignances if detected early. Screening for breast cancer with digital mammography (DM) has been widely used. However it demonstrates limited sensitivity for women with dense breasts. An emerging technology in the field is contrast-enhanced digital mammography (CEDM), which includes a low energy (LE) image similar to DM, and a recombined image leveraging tumor neoangiogenesis similar to breast magnetic resonance imaging (MRI). CEDM has shown better diagnostic accuracy than DM. While promising, CEDM is not yet widely available across medical centers. In this research, we propose a Shallow-Deep Convolutional Neural Network (SD-CNN) where a shallow CNN is developed to derive "virtual" recombined images from LE images, and a deep CNN is employed to extract novel features from LE, recombined or "virtual" recombined images for ensemble models to classify the cases as benign vs. cancer. To evaluate the validity of our approach, we first develop a deep-CNN using 49 CEDM cases collected from Mayo Clinic to prove the contributions from recombined images for improved breast cancer diagnosis (0.86 in accuracy using LE imaging vs. 0.90 in accuracy using both LE and recombined imaging). We then develop a shallow-CNN using the same 49 CEDM cases to learn the nonlinear mapping from LE to recombined images. Next, we use 69 DM cases collected from the hospital located at Zhejiang University, China to generate "virtual" recombined images. Using DM alone provides 0.91 in accuracy, whereas SD-CNN improves the diagnostic accuracy to 0.95.},
	urldate = {2018-10-11},
	journal = {arXiv:1803.00663 [cs]},
	author = {Gao, Fei and Wu, Teresa and Li, Jing and Zheng, Bin and Ruan, Lingxiang and Shang, Desheng and Patel, Bhavika},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.00663},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1803.00663 PDF:/home/marcelohpf/Zotero/storage/TPPR2MIS/Gao et al. - 2018 - SD-CNN a Shallow-Deep CNN for Improved Breast Can.pdf:application/pdf;arXiv.org Snapshot:/home/marcelohpf/Zotero/storage/V7MHLTC3/1803.html:text/html}
}

@article{graves_framewise_2005,
	series = {{IJCNN} 2005},
	title = {Framewise phoneme classification with bidirectional {LSTM} and other neural network architectures},
	volume = {18},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608005001206},
	doi = {10.1016/j.neunet.2005.06.042},
	abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it.11An abbreviated version of some portions of this article appeared in (Graves and Schmidhuber, 2005), as part of the IJCNN 2005 conference proceedings, published under the IEEE copyright.},
	number = {5},
	urldate = {2018-10-24},
	journal = {Neural Networks},
	author = {Graves, Alex and Schmidhuber, Jürgen},
	month = jul,
	year = {2005},
	pages = {602--610},
	file = {ScienceDirect Full Text PDF:/home/marcelohpf/Zotero/storage/TJIAR75B/Graves e Schmidhuber - 2005 - Framewise phoneme classification with bidirectiona.pdf:application/pdf;ScienceDirect Snapshot:/home/marcelohpf/Zotero/storage/KD8WXZLM/S0893608005001206.html:text/html}
}

@article{hochreiter_long_1997,
	title = {Long short-term memory},
	volume = {9},
	issn = {0899-7667},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {eng},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, S. and Schmidhuber, J.},
	month = nov,
	year = {1997},
	pmid = {9377276},
	keywords = {Algorithms, Learning, Memory, Memory, Short-Term, Models, Neurological, Models, Psychological, Nerve Net, Neural Networks (Computer), Time Factors},
	pages = {1735--1780},
	file = {2604.pdf:/home/marcelohpf/Zotero/storage/9LTCP555/2604.pdf:application/pdf;IEEE Xplore Abstract Record:/home/marcelohpf/Zotero/storage/NSUGGEJV/6795963.html:text/html}
}

@article{schuster_bidirectional_1997,
	title = {Bidirectional recurrent neural networks},
	volume = {45},
	issn = {1053-587X},
	doi = {10.1109/78.650093},
	abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.},
	number = {11},
	journal = {IEEE Transactions on Signal Processing},
	author = {Schuster, M. and Paliwal, K. K.},
	month = nov,
	year = {1997},
	keywords = {Databases, pattern classification, speech recognition, Speech recognition, Recurrent neural networks, statistical analysis, artificial data, Artificial neural networks, bidirectional recurrent neural networks, classification experiments, complete symbol sequences, conditional posterior probability, Control systems, learning by example, learning from examples, negative time direction, Parameter estimation, phonemes, positive time direction, Probability, real data, recurrent neural nets, regression experiments, regular recurrent neural network, Shape, speech processing, Telecommunication control, TIMIT database, training, Training data},
	pages = {2673--2681},
	file = {IEEE Xplore Abstract Record:/home/marcelohpf/Zotero/storage/IED3D7UD/650093.html:text/html;IEEE Xplore Full Text PDF:/home/marcelohpf/Zotero/storage/FH2LTIXW/Schuster e Paliwal - 1997 - Bidirectional recurrent neural networks.pdf:application/pdf}
}

@incollection{zhang_character-level_2015,
	title = {Character-level {Convolutional} {Networks} for {Text} {Classification}},
	url = {http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf},
	urldate = {2018-10-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {649--657},
	file = {NIPS Full Text PDF:/home/marcelohpf/Zotero/storage/J4N3ML4Z/Zhang et al. - 2015 - Character-level Convolutional Networks for Text Cl.pdf:application/pdf;NIPS Snapshot:/home/marcelohpf/Zotero/storage/P8YXLP6W/5782-character-level-convolutional-networks-for-text-classification.html:text/html}
}

@inproceedings{yang_hierarchical_2016,
	address = {San Diego, California},
	title = {Hierarchical {Attention} {Networks} for {Document} {Classification}},
	url = {http://aclweb.org/anthology/N16-1174},
	doi = {10.18653/v1/N16-1174},
	abstract = {We propose a hierarchical attention network for document classiﬁcation. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classiﬁcation tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
	language = {en},
	urldate = {2018-10-26},
	booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
	year = {2016},
	pages = {1480--1489},
	file = {Yang et al. - 2016 - Hierarchical Attention Networks for Document Class.pdf:/home/marcelohpf/Zotero/storage/FXXFX3CL/Yang et al. - 2016 - Hierarchical Attention Networks for Document Class.pdf:application/pdf}
}

@inproceedings{graves_hybrid_2013,
	address = {Olomouc, Czech Republic},
	title = {Hybrid speech recognition with {Deep} {Bidirectional} {LSTM}},
	isbn = {978-1-4799-2756-2},
	url = {http://ieeexplore.ieee.org/document/6707742/},
	doi = {10.1109/ASRU.2013.6707742},
	urldate = {2018-10-25},
	booktitle = {2013 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding}},
	publisher = {IEEE},
	author = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
	month = dec,
	year = {2013},
	pages = {273--278},
	file = {Hybrid speech recognition with Deep Bidirectional .pdf:/home/marcelohpf/Zotero/storage/WWA4ACZC/Hybrid speech recognition with Deep Bidirectional .pdf:application/pdf}
}

@article{smola_tutorial_2004,
	title = {A tutorial on support vector regression},
	volume = {14},
	issn = {1573-1375},
	url = {https://doi.org/10.1023/B:STCO.0000035301.49549.88},
	doi = {10.1023/B:STCO.0000035301.49549.88},
	abstract = {In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.},
	language = {en},
	number = {3},
	urldate = {2018-10-30},
	journal = {Statistics and Computing},
	author = {Smola, Alex J. and Sch{\" o}lkopf, Bernhard},
	month = aug,
	year = {2004},
	keywords = {machine learning, support vector machines, regression estimation},
	pages = {199--222},
}

@inproceedings{conneau_very_2017,
	address = {Valencia, Spain},
	title = {Very {Deep} {Convolutional} {Networks} for {Text} {Classification}},
	url = {http://aclweb.org/anthology/E17-1104},
	doi = {10.18653/v1/E17-1104},
	abstract = {The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-ofthe-art on several public text classiﬁcation tasks. To the best of our knowledge, this is the ﬁrst time that very deep convolutional nets have been applied to text processing.},
	language = {en},
	urldate = {2018-11-10},
	booktitle = {Proceedings of the 15th {Conference} of the {European} {Chapter} of the           {Association} for {Computational} {Linguistics}: {Volume} 1, {Long} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Schwenk, Holger and Barrault, Loïc and Lecun, Yann},
	year = {2017},
	pages = {1107--1116},
	file = {Conneau et al. - 2017 - Very Deep Convolutional Networks for Text Classifi.pdf:/home/marcelohpf/Zotero/storage/YT7INWUX/Conneau et al. - 2017 - Very Deep Convolutional Networks for Text Classifi.pdf:application/pdf}
}

@inproceedings{kim_convolutional_2014,
	address = {Doha, Qatar},
	title = {Convolutional {Neural} {Networks} for {Sentence} {Classification}},
	url = {http://aclweb.org/anthology/D14-1181},
	doi = {10.3115/v1/D14-1181},
	abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classiﬁcation tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-speciﬁc vectors through ﬁne-tuning offers further gains in performance. We additionally propose a simple modiﬁcation to the architecture to allow for the use of both task-speciﬁc and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classiﬁcation.},
	language = {en},
	urldate = {2018-11-10},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Yoon},
	year = {2014},
	pages = {1746--1751},
	file = {Kim - 2014 - Convolutional Neural Networks for Sentence Classif.pdf:/home/marcelohpf/Zotero/storage/38KU4TC5/Kim - 2014 - Convolutional Neural Networks for Sentence Classif.pdf:application/pdf}
}

@article{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	volume = {abs/1310.4546},
	url = {http://arxiv.org/abs/1310.4546},
	journal = {CoRR},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	year = {2013}
}

@inproceedings{mikolov_linguistic_2013,
	title = {Linguistic regularities in continuous space word representations},
	booktitle = {Proceedings of the 2013 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
	year = {2013},
	pages = {746--751}
}

@article{rodriguez_quality_2016,
	title = {Quality performance metrics in multivariate classification methods for qualitative analysis},
	volume = {80},
	issn = {0165-9936},
	doi = {https://doi.org/10.1016/j.trac.2016.04.021},
	journal = {TrAC Trends in Analytical Chemistry},
	author = {Rodr{\' i}guez, Luis Cuadros and Casta{\~ n}o, Estefan{\' i}a P{\' e}rez and Sambl{\' a}s, Cristina Ruiz},
	year = {2016},
	pages = {612--624}
}

@book{prodanov_metodologia_2013,
	address = {Novo Hamburgo, RS},
	edition = {2ª},
	title = {Metodologia do trabalho cientifico: {Métodos} e {Técnicas} da {Pesquisa} e do {Trabalho} {Acadêmico}},
	isbn = {978-85-7717-158-3},
	publisher = {Editora Feevale},
	author = {Prodanov, Cleber Cristiano and Freitas, Ernani Cesar de},
	year = {2013}
}

@book{goncalves_direito_2016,
	address = {São Paulo},
	edition = {6},
	title = {Direito processual civil esquematizado},
	publisher = {Saraiva},
	author = {Gon{\c c}alves, Marcus Vinicius Rios},
	year = {2016}
}

@book{aes_dicionario_2012,
	address = {São Paulo},
	edition = {16},
	title = {Dicionário compacto jurídico},
	isbn = {978-85-339-2023-1},
	publisher = {Rideel},
	author = {aes, Deocleciano Torrieri Guimar∼},
	year = {2012}
}

@book{gil_como_2002,
	address = {São Paulo, SP},
	edition = {4ª},
	title = {Como elaborar projetos de pesquisa},
	isbn = {85-224-3169-8},
	publisher = {Atlas S.A.},
	author = {Gil, Antonio Carlos},
	year = {2002}
}

@book{manning_introduction_2008,
	address = {New York, NY, USA},
	title = {Introduction to {Information} {Retrieval}},
	isbn = {0-521-86571-9 978-0-521-86571-5},
	publisher = {Cambridge University Press},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\" u}tze, Hinrich},
	year = {2008}
}

@book{nielsen_neural_2015,
	title = {Neural networks and deep learning},
	url = {http://neuralnetworksanddeeplearning.com/chap2.html},
	publisher = {Determination Press},
	author = {Nielsen, Michael A},
	year = {2015}
}

@book{brink_real-world_2015,
	edition = {MEAP},
	title = {Real-{World} {Machine} {Learning}},
	number = {10},
	publisher = {Manning},
	author = {Brink, Henrik and Richards, Joseph W. and Fetherolf, Mark},
	year = {2015}
}

@book{goyvaerts_regular_2012,
	edition = {Second},
	title = {Regular {Expressions} {Cookbook}},
	isbn = {978-1-4493-1943-4},
	publisher = {O'Reilly Media},
	author = {Goyvaerts, Jan and Levithan, Steven},
	year = {2012}
}

@book{goldberg_neural_2017,
	title = {Neural {Network} {Methods} for {Natural} {Language} {Processing}},
	isbn = {978-1-62705-295-5},
	publisher = {Morgan \& Claypool},
	author = {Goldberg, Yoav},
	editor = {Hirst, Graeme},
	year = {2017},
	doi = {10.2200/S00762ED1V01Y201703HLT037}
}

@book{amendoeira_jr_manual_2012,
	address = {São Paulo},
	edition = {2},
	title = {Manual de direito processual civil: {Teoria} geral do processo e fase de conhecimento em primeiro grau de jurisdição},
	volume = {1},
	isbn = {978-85-02-12710-4},
	publisher = {Editora Saraiva},
	author = {{AMENDOEIRA JR}, Sidnei},
	year = {2012}
}

@incollection{ruschel_governo_2011,
	address = {Zaragoza, Espanha},
	series = {{LEFIS} series; 13},
	title = {Governo eletrônico: o judiciário na era do acesso},
	isbn = {978-84-15274-66-7},
	booktitle = {{CALLEJA}, {P}.{L}. ({Org}). {La} {Administración} {Electrónica} como {Herramienta} de {Inclusión} {Digital}},
	publisher = {Zaragoza: Prensas Universitarias de Zaragoza},
	author = {Ruschel, Airton José and Rover, Aires José and Schneider, Juliete},
	year = {2011},
	pages = {59 -- 79}
}

@article{hearst_support_1998,
	title = {Support vector machines},
	volume = {13},
	number = {4},
	journal = {IEEE Intelligent Systems and their applications},
	author = {Hearst, Marti A. and Dumais, Susan T and Osuna, Edgar and Platt, John and Scholkopf, Bernhard},
	year = {1998},
	pages = {18--28}
}

@article{singh_text_2016,
	title = {Text {Stemming}: {Approaches}, {Applications}, and {Challenges}},
	volume = {49},
	issn = {0360-0300},
	url = {http://doi-acm-org.ez54.periodicos.capes.gov.br/10.1145/2975608},
	doi = {10.1145/2975608},
	number = {3},
	journal = {ACM Comput. Surv.},
	author = {Singh, Jasmeet and Gupta, Vishal},
	month = sep,
	year = {2016},
	keywords = {corpus-based stemming, morphological analysis, rule-based stemming, statistical stemming, stemmers, Text stemming},
	pages = {45:1--45:46}
}

@article{pedregosa_scikit-learn:_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	pages = {2825--2830}
}

@article{oliveira_automatic_2017,
	title = {Automatic classification of journalistic documents on the {Internet}},
	volume = {29},
	issn = {0103-3786},
	url = {http://www.scielo.br/scielo.php?script=sci_arttext&pid=S0103-37862017000300245&nrm=iso},
	language = {en},
	journal = {Transinformação},
	author = {OLIVEIRA, Elias and FILHO, Delermando BRANQUINHO},
	year = {2017},
	pages = {245 -- 255}
}

@inproceedings{mikolov_recurrent_2010,
	title = {Recurrent neural network based language model},
	booktitle = {Eleventh {Annual} {Conference} of the {International} {Speech} {Communication} {Association}},
	author = {Mikolov, Tomáš and Karafiát, Martin and Burget, Lukáš and Černockỳ, Jan and Khudanpur, Sanjeev},
	year = {2010}
}

@inproceedings{eslick_langutils:_2005,
	address = {Stanford, California},
	title = {Langutils: {A} {Natural} {Language} {Toolkit} for {Common} {Lisp}},
	booktitle = {Proceedings of the {International} {Conference} on {Lisp}},
	author = {Eslick, Ian and Liu, Hugo},
	year = {2005}
}

@inproceedings{crowston_comparing_2017,
	address = {Mānoa, Hawaii},
	title = {Comparing {Data} {Science} {Project} {Management} {Methodologies} via a {Controlled} {Experiment}},
	isbn = {978-0-9981331-0-2},
	doi = {10.24251/HICSS.2017.120},
	booktitle = {Proceedings of the 50th {Hawaii} {International} {Conference} on {System} {Sciences}},
	author = {Crowston, Kevin and Saltz, Jeffrey S. and Shamshurin, Ivan},
	year = {2017},
	pages = {10}
}

@inproceedings{pennington_glove:_2014,
	title = {{GloVe}: {Global} {Vectors} for {Word} {Representation}},
	url = {http://www.aclweb.org/anthology/D14-1162},
	booktitle = {Empirical {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
	year = {2014},
	pages = {1532--1543}
}

@book{chapman_crisp-dm_2000,
	title = {{CRISP}-{DM} 1.0: {Step}-by-step data mining guide},
	url = {https://www.the-modeling-agency.com/crisp-dm.pdf},
	publisher = {CRISP-DM consortium},
	author = {Chapman, Pete and Clinton, Julian and Kerber, Randy and Khabaza, Thomas and Reinartz, Thomas and Shearer, Colin and Wirth, Rüdiger},
	year = {2000}
}

@book{brasil_constituicao_1988,
	title = {Constituição da {República} {Federativa} do {Brasil}},
	publisher = {Senado},
	author = {Brasil},
	year = {1988}
}

@book{brasil_lei_2015,
	title = {Lei {Nº} 13.105, de 16 de março de 2015. {Código} de {Processo} {Civil}},
	url = {www.planalto.gov.br/ccivil_03/_ato2015-2018/2015/lei/l13105.htm},
	author = {Brasil},
	year = {2015}
}

@manual{noauthor_regimento_2016,
	address = {Brasília, DF},
	title = {Regimento {Interno}},
	org-Short={BRASIL},
	organization = {BRASIL. {Supremo Tribunal Federal}},
	year = {2016}
}

@manual{noauthor_resolucao_2010,
	address = {Brasília, DF},
	title = {Resolução {Nº} 427, de 20 de abril de 2010},
	org-Short={BRASIL},
	organization = {BRASIL. {Supremo Tribunal Federal}},
	year = {2010}
}

@manual{noauthor_lei_2012,
	address = {Brasília, DF},
	title = {Lei {Nº} 12.665, de 13 de junho de 2012},
	url = {https://www.planalto.gov.br/ccivil_03/_ato2011-2014/2012/lei/l12665.htm},
	org-Short = {BRASIL},
	organization={BRASIL},
	year = {2012}
}

@book{noauthor_lei_1992,
	address = {Brasília, DF},
	title = {Lei {Nº} 8.457, de 4 de setembro de 1992},
	url = {https://www.planalto.gov.br/ccivil_03/leis/l8457.htm},
	organization = {BRASIL},
	org-Short = {BRASIL},
	year = {1992}
}

@misc{supremo_tribunal_federal_inteligencia_2018,
	title = {Inteligência artificial vai agilizar a tramitação de processos no {STF}},
	url = {http://www.stf.jus.br/portal/cms/verNoticiaDetalhe.asp?idConteudo=380038},
	journal = {STF},
	organization = {BRASIL. {Supremo Tribunal Federal}},
	org-Short={BRASIL},
	month = may,
	year = {2018}
}

@book{noauthor_termo_2009,
	address = {Brasília, DF},
	title = {Termo de acordo de cooperação técnica {Nº} 058/2009},
	organization = {BRASIL. {Conselho Nacional de Justiça}},
	org-Short={BRASIL},
	year = {2009}
}

@book{noauthor_lei_2011,
	address = {Brasília, DF},
	title = {Lei {Nº} 12.527, de 18 de novembro de 2011},
	organization = {BRASIL. {Congresso Nacional}},
	org-Short = {BRASIL},
	year = {2011}
}

@misc{noauthor_wiki_2018,
    author = {{Wiki PJE}},
	title = {Wiki {PJe}},
	url = {http://www.pje.jus.br/wiki/index.php/P\'agina_principal},
	year = {2018}
}

@misc{raschka_single-layer_2015,
	title = {Single-{Layer} {Neural} {Networks} and {Gradient} {Descent}},
	url = {https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html},
	author = {Raschka, Sebastian},
	year = {2015}
}

@article{noauthor_recent_2018,
	title = {Recent advances in convolutional neural networks},
	volume = {77},
	issn = {0031-3203},
	url = {https://www-sciencedirect.ez54.periodicos.capes.gov.br/science/article/pii/S0031320317304120},
	doi = {10.1016/j.patcog.2017.10.013},
	abstract = {In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and nat…},
	language = {en},
	urldate = {2018-11-16},
	journal = {Pattern Recognition},
	month = may,
	year = {2018},
	pages = {354--377},
	file = {Snapshot:/home/marcelohpf/Zotero/storage/W2ARZFJ2/S0031320317304120.html:text/html;Submitted Version:/home/marcelohpf/Zotero/storage/NI5KD4K2/2018 - Recent advances in convolutional neural networks.pdf:application/pdf}
}

@article{sachan_investigating_nodate,
	title = {Investigating the {Working} of {Text} {Classifiers}},
	abstract = {Text classiﬁcation is one of the most widely studied tasks in natural language processing. Motivated by the principle of compositionality, large multilayer neural network models have been employed for this task in an attempt to effectively utilize the constituent expressions. Almost all of the reported work train large networks using discriminative approaches, which come with a caveat of no proper capacity control, as they tend to latch on to any signal that may not generalize. Using various recent state-of-the-art approaches for text classiﬁcation, we explore whether these models actually learn to compose the meaning of the sentences or still just focus on some keywords or lexicons for classifying the document. To test our hypothesis, we carefully construct datasets where the training and test splits have no direct overlap of such lexicons, but overall language structure would be similar. We study various text classiﬁers and observe that there is a big performance drop on these datasets. Finally, we show that even simple models with our proposed regularization techniques, which disincentivize focusing on key lexicons, can substantially improve classiﬁcation accuracy.},
	language = {en},
	author = {Sachan, Devendra and Zaheer, Manzil and Salakhutdinov, Ruslan},
	pages = {12},
	file = {Sachan et al. - Investigating the Working of Text Classifiers.pdf:/home/marcelohpf/Zotero/storage/N5L6VNQG/Sachan et al. - Investigating the Working of Text Classifiers.pdf:application/pdf}
}

@inproceedings{tang_document_2015,
	address = {Lisbon, Portugal},
	title = {Document {Modeling} with {Gated} {Recurrent} {Neural} {Network} for {Sentiment} {Classification}},
	url = {http://aclweb.org/anthology/D15-1167},
	doi = {10.18653/v1/D15-1167},
	urldate = {2018-11-16},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Tang, Duyu and Qin, Bing and Liu, Ting},
	year = {2015},
	pages = {1422--1432},
	file = {Full Text PDF:/home/marcelohpf/Zotero/storage/SS38IN6V/Tang et al. - 2015 - Document Modeling with Gated Recurrent Neural Netw.pdf:application/pdf}
}

@article{zhou_c-lstm_2015,
	title = {A {C}-{LSTM} {Neural} {Network} for {Text} {Classification}},
	url = {https://arxiv.org/abs/1511.08630},
	language = {en},
	urldate = {2018-11-16},
	author = {Zhou, Chunting and Sun, Chonglin and Liu, Zhiyuan and Lau, Francis C. M.},
	month = nov,
	year = {2015},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:/home/marcelohpf/Zotero/storage/II7VYUCY/Zhou et al. - 2015 - A C-LSTM Neural Network for Text Classification.pdf:application/pdf;Snapshot:/home/marcelohpf/Zotero/storage/H4CLN8J6/1511.html:text/html;Zhou et al. - 2015 - A C-LSTM Neural Network for Text Classification.pdf:/home/marcelohpf/Zotero/storage/6LCKABDM/Zhou et al. - 2015 - A C-LSTM Neural Network for Text Classification.pdf:application/pdf}
}

@article{wang_baselines_2012,
	title = {Baselines and {Bigrams}: {Simple}, {Good} {Sentiment} and {Topic} {Classification}},
	abstract = {Variants of Naive Bayes (NB) and Support Vector Machines (SVM) are often used as baseline methods for text classiﬁcation, but their performance varies greatly depending on the model variant, features used and task/ dataset. We show that: (i) the inclusion of word bigram features gives consistent gains on sentiment analysis tasks; (ii) for short snippet sentiment tasks, NB actually does better than SVMs (while for longer documents the opposite result holds); (iii) a simple but novel SVM variant using NB log-count ratios as feature values consistently performs well across tasks and datasets. Based on these observations, we identify simple NB and SVM variants which outperform most published results on sentiment analysis datasets, sometimes providing a new state-of-the-art performance level.},
	language = {en},
	number = {50},
	journal = {50th Annual Meeting of the Association for Computational Linguistics},
	author = {Wang, Sida and Manning, Christopher},
	year = {2012},
	pages = {5},
	file = {Wang e Manning - Baselines and Bigrams Simple, Good Sentiment and .pdf:/home/marcelohpf/Zotero/storage/VVSLBY6S/Wang e Manning - Baselines and Bigrams Simple, Good Sentiment and .pdf:application/pdf}
}

@article{rosenblatt_perceptron:_1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {1939-1471, 0033-295X},
	shorttitle = {The perceptron},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},
	doi = {10.1037/h0042519},
	language = {en},
	number = {6},
	urldate = {2018-11-18},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	pages = {386--408},
	file = {Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:/home/marcelohpf/Zotero/storage/J9Y8ZGTA/Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:application/pdf}
}

@article{enriquez_approach_2016,
	title = {An {Approach} to the {Use} of {Word} {Embeddings} in an {Opinion} {Classification} {Task}},
	volume = {66},
	issn = {0957-4174},
	url = {https://doi.org/10.1016/j.eswa.2016.09.005},
	doi = {10.1016/j.eswa.2016.09.005},
	abstract = {Vector-based word representations can help to improve a document classifier.The information of word2vec vectors and bags of words are very complementary.The combination of word2vec and BOW word representations obtains the best results.Word2vec is much more stable than bag of words models in cross-domain experiments. In this paper we show how a vector-based word representation obtained via word2vec can help to improve the results of a document classifier based on bags of words. Both models allow obtaining numeric representations from texts, but they do it very differently. The bag of words model can represent documents by means of widely dispersed vectors in which the indices are words or groups of words. word2vec generates word level representations building vectors that are much more compact, where indices implicitly contain information about the context of word occurrences. Bags of words are very effective for document classification and in our experiments no representation using only word2vec vectors is able to improve their results. However, this does not mean that the information provided by word2vec is not useful for the classification task. When this information is used in combination with the bags of words, the results are improved, showing its complementarity and its contribution to the task. We have also performed cross-domain experiments in which word2vec has shown much more stable behavior than bag of words models.},
	number = {C},
	urldate = {2018-11-18},
	journal = {Expert Syst. Appl.},
	author = {Enríquez, Fernando and Troyano, José A. and López-Solaz, Tomás},
	month = dec,
	year = {2016},
	keywords = {Bag of words, Document classification, Opinion classification, Word embedding, Word2vec},
	pages = {1--6}
}

@article{butnaru_image_2017,
	title = {From {Image} to {Text} {Classification}},
	volume = {112},
	issn = {1877-0509},
	url = {https://doi.org/10.1016/j.procs.2017.08.211},
	doi = {10.1016/j.procs.2017.08.211},
	abstract = {In this paper, we propose a novel approach for text classification based on clustering word embeddings, inspired by the bag of visual words model, which is widely used in computer vision. After each word in a collection of documents is represented as word vector using a pre-trained word embeddings model, a k-means algorithm is applied on the word vectors in order to obtain a fixed-size set of clusters. The centroid of each cluster is interpreted as a super word embedding that embodies all the semantically related word vectors in a certain region of the embedding space. Every embedded word in the collection of documents is then assigned to the nearest cluster centroid. In the end, each document is represented as a bag of super word embeddings by computing the frequency of each super word embedding in the respective document. We also diverge from the idea of building a single vocabulary for the entire collection of documents, and propose to build class-specific vocabularies for better performance. Using this kind of representation, we report results on two text mining tasks, namely text categorization by topic and polarity classification. On both tasks, our model yields better performance than the standard bag of words.},
	number = {C},
	urldate = {2018-11-18},
	journal = {Procedia Comput. Sci.},
	author = {Butnaru, Andrei M. and Ionescu, Radu Tudor},
	month = sep,
	year = {2017},
	keywords = {bag of super word embeddings, bag of word embeddings, bag of words, clustering word embeddings, k-means, kernel methods, polarity classification, text categorization, word embeddings},
	pages = {1783--1792},
	file = {Submitted Version:/home/marcelohpf/Zotero/storage/FEP5VLUN/Butnaru e Ionescu - 2017 - From Image to Text Classification.pdf:application/pdf}
}

@inproceedings{ge_improving_2017,
	title = {Improving text classification with word embedding},
	doi = {10.1109/BigData.2017.8258123},
	abstract = {One challenge in text classification is that it is difficult to make feature reductions based on the definition of the features. An ineffective feature reduction may even worsen the classification accuracy. Word2Vec, a word embedding method, has recently been gaining popularity due to its high precision rate of analyzing the semantic similarity between words at relatively low computational cost. However, there is limited research about feature reduction using Word2Vec. In this project, we developed a method using Word2Vec to reduce the feature size while increasing the classification accuracy. We achieved feature reduction by loosely clustering similar features using graph search techniques. Similarity thresholds above 0.5 were used in our method to pair and cluster the features. Finally, we utilized Multinomial Naïve Bayes classifier, Support Vector Machine, K Nearest Neighbor and Random Forest classifier to evaluate the effect of our method. Four datasets with dimensions up to 100,000 feature size and 400,000 document size were used to evaluate the result of our method. The result showed that around 4-10\% feature reduction was achieved with up to 1-4\% improvement of classification accuracy in terms of different datasets and classifiers. Meanwhile, we also succeeded in improving feature reduction and classification accuracy by combining our method with other classic feature reduction techniques such as chi-square and mutual information.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Ge, L. and Moh, T.},
	month = dec,
	year = {2017},
	keywords = {text classification, Computational modeling, learning (artificial intelligence), Machine learning, classification accuracy, feature extraction, Feature extraction, pattern classification, support vector machines, Support vector machines, Support Vector Machine, Semantics, text analysis, Word embedding, Bayes methods, classic feature reduction techniques, Computational efficiency, feature reduction, Feature reduction, feature size, ineffective feature reduction, K Nearest Neighbor, KNN, Multinomial Naïve Bayes classifier, Naïve Bayes, Ontologies, Random Forest classifier, SVM, Text categorization, word embedding method, word processing, Word2Vec},
	pages = {1796--1805},
	file = {IEEE Xplore Abstract Record:/home/marcelohpf/Zotero/storage/SKEK6S8M/8258123.html:text/html}
}

@book{guimaraes_dicionario_2012,
	address = {São Paulo, SP},
	edition = {16},
	title = {Dicionário compacto jurídico},
	isbn = {978-85-339-2023-1},
	publisher = {Rideel},
	author = {Guimar{\~ a}es, Deocleciano Torrieri},
	year = {2012}
}

@inproceedings{lai_recurrent_2015,
	address = {Austin, Texas},
	series = {{AAAI}'15},
	title = {Recurrent {Convolutional} {Neural} {Networks} for {Text} {Classification}},
	isbn = {978-0-262-51129-2},
	url = {http://dl.acm.org/citation.cfm?id=2886521.2886636},
	abstract = {Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classification without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classification to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets.},
	urldate = {2018-11-18},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
	year = {2015},
	pages = {2267--2273},
	file = {Lai et al. - Recurrent Convolutional Neural Networks for Text C.pdf:/home/marcelohpf/Zotero/storage/ZJUN9B5S/Lai et al. - Recurrent Convolutional Neural Networks for Text C.pdf:application/pdf}
}

@inproceedings{tang_document_2015-1,
	address = {Lisbon, Portugal},
	title = {Document {Modeling} with {Gated} {Recurrent} {Neural} {Network} for {Sentiment} {Classification}},
	url = {http://aclweb.org/anthology/D15-1167},
	doi = {10.18653/v1/D15-1167},
	language = {en},
	urldate = {2018-11-19},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Tang, Duyu and Qin, Bing and Liu, Ting},
	year = {2015},
	pages = {1422--1432},
	file = {Tang et al. - 2015 - Document Modeling with Gated Recurrent Neural Netw.pdf:/home/marcelohpf/Zotero/storage/EN3KJ487/Tang et al. - 2015 - Document Modeling with Gated Recurrent Neural Netw.pdf:application/pdf}
}



@inproceedings{braz_document_2018,
	address = {Montreal, CA},
	title = {Document classification using a {Bi}-{LSTM} to unclog {Brazil}'s supreme court},
	language = {English},
	booktitle = {32st {Conference} on {Neural} {Information} {Processing} {Systems} ({NIPS})},
	author = {Braz, F. A. and da Silva, N. Correia and de Campos, T. E. and Chaves, F. B. and Ferreira, M. H. P. and de Almeida, A. P. G. S. and Vidal, F. and Inazawa, P. H. G. and Coelho, V. H. D. and Sukiennik, B. P. and Bezerra, D. Alves and Gusmão, D. B. and Ziegler, G. G. and Fernes, R. V. C. and Peixoto, F. Hartman},
	month = dec,
	year = {2018}
}


@article{srivastava_dropout:_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	issn = {1532-4435},
	shorttitle = {Dropout},
	url = {http://dl.acm.org/citation.cfm?id=2627435.2670313},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	number = {1},
	urldate = {2018-11-25},
	journal = {J. Mach. Learn. Res.},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	month = jan,
	year = {2014},
	keywords = {deep learning, model combination, neural networks, regularization},
	pages = {1929--1958},
	file = {ACM Full Text PDF:/home/marcelohpf/Zotero/storage/SVXX8CQC/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf}
}

@inproceedings{da_silva_document_2018,
	address = {São Paulo, SP},
	title = {Document type classification for {Brazil}'s supreme court using a {Convolutional} {Neural} {Network}},
	url = {http://icofcs.org/2018/papers-accepted.html},
	booktitle = {10th {International} {Conference} on {Forensic} {Computer} {Science} and {Cyber} {Law}},
	author = {da Silva, Nilton Correia and Braz, Fabrício Ataídes and de Campos, T. E. and Gusmao, D.B. and Chaves, F.B. and Mendes, D.B. and Bezerra, D.A. and Ziegler, G.G. and Horinouchi, L.H. and Ferreira, M.H.P. and Carvalho, G.H.T.A. and Fernandes, R. V. C. and Peixoto, F. H. and Maia Filho, M. S. and Sukiennik, B. P. and Rosa, L. S. and Silva, R. Z. M. and Junquilho, T. A.},
	month = oct,
	year = {2018},
	pages = {4},
	file = {correiaDaSilva_etal_icofcs2018.pdf:/home/marcelohpf/Zotero/storage/ISIS7K4H/correiaDaSilva_etal_icofcs2018.pdf:application/pdf}
}