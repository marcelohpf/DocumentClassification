\begin{apendicesenv}

\partapendices
\chapter{limpeza dos dados}
\label{sec:apendiceA}

O código abaixo foi utilizado para aplicar as expressões regulares, as normalizações e a transformação em símbolos. Este código foi desenvolvido pelo Grupo de Pesquisa em Aprendizado de Máquina da Universidade de Brasília e foi adicionado aqui por estar em um repositório de código privado.

\begin{lstlisting}[language=Python,extendedchars=true]
import re
from spacy.lang import pt
import nltk
from nltk.stem.snowball import SnowballStemmer

try:
    nltk.word_tokenize('some word')
except:
    nltk.download('punkt')

try:
    nltk.corpus.stopwords.words('portuguese')
except:
    nltk.download('stopwords')
finally:
    STOP_WORDS = pt.STOP_WORDS.union(
        set(nltk.corpus.stopwords.words('portuguese'))
    )


class CorpusHandler:

    def __init__(self):
        pass

    @staticmethod
    def clean_number(document, **kwargs):
        return re.sub(r'\s\d+\s', ' ', document)

    @staticmethod
    def clean_email(document, **kwargs):
        local_part = r"[0-9a-zA-Z!#$%&'*+-/=?^_`{|}~.]+"
        domain = r"[a-zA-Z][a-zA-Z0-9-.]*[a-zA-Z]\.\w{2,4}"
        document = re.sub(r"\s{}@{}".format(
                local_part, domain),
            ' EMAIL ', document)
        return document

    @staticmethod
    def clean_site(document, **kwargs):
        # Addaption for rules of
        # https://tools.ietf.org/html/rfc3986#section-3 to
        # stf's documents
        scheme = r"[a-zA-Z][a-zA-Z0-9+-.]*:?//?"
        host_ip = r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}'
        host_name = r"[a-zA-Z]+(\.[a-zA-Z0-9-_~]+)+"
        www = r'www\.{}'.format(host_name)
        scheme_host = r'({}{}|{}{}|{}{}|{})'.format(
            scheme, host_ip, scheme, www, scheme, host_name, www
        )
        port = r"(:\d+)?"
        resource = r"(/[a-zA-Z0-9-._~!$&'/*+,;=]*)?"
        query = r"(\?[a-zA-Z0-9-._~!$&'/*+,;=]*)?"
        fragment = r"(#[a-zA-Z0-9-._~!$&'/*+,;=]*)?"
        document = re.sub(r"{}{}{}{}{}".format(
                scheme_host, port, resource, query, fragment),
            ' SITE ', document)
        return document

    @staticmethod
    def transform_token(document, **kwargs):
        word_number = r'(n.?|numero|numero|n.?|no.?)?'
        # Law number: 00/YEAR or 00
        number_law = r'([0-9]+)((\s|\.)+\d+)?'
        matchs = [('LEI', 'lei'), ('ARTIGO', r'art\.|art\w*'),
                  ('DECRETO', 'decreto'),]

        for word, regex in matchs:
            document = re.sub(r'{}\s*{}\s*{}'.format(
                    regex, word_number, number_law),
                r'{}_\2'.format(word),
                document, flags=re.I)

        return document

    @staticmethod
    def remove_small_big_words(document, **kwargs):
        # remove 2 chars
        document = re.sub(r'\s\w{0,2}\s', ' ', document)
        # remove bigger words ex.: infrainconstitucionalidade
        document = re.sub(r'\s\w{30,}\s', ' ', document)
        return document

    @staticmethod
    def remove_letter_number(document, **kwargs):
        # Remove wor00 00wo 00wor00 wo00rd and keep WORD_000
        return re.sub(r'([A-Z]+_\d+)|[^ ]*\d+[^ ]*', r'\1', document)

    @staticmethod
    def clean_document(document, **kwargs):
        # Replace 0.0 for 00
        document = re.sub(r'(\d)\.(\d)', r'\1\2', document)

        # Remove all non alphanumeric
        document = re.sub(r'\W', ' ', document)

        return document

    @staticmethod
    def clean_spaces(document, **kwargs):
        # Remove multiple spaces
        document = re.sub(r'\s+', ' ', document)
        document = document.strip()
        return document

    @staticmethod
    def clean_alphachars(document, **kwargs):
        return re.sub(r'[^ ]*[^-_\'u\'i\'o\~o\^o\'e\^e\~a'
          '\'a\`a\^aa-z0-9\{\c c} ]+[^ ]*',
                    ' ', document)

    @staticmethod
    def tokenize(document, **kwargs):
        """Transform string documents in array of tokens"""
        if isinstance(document, str):
            document = document.split()
        return document

    @classmethod
    def remove_stop_words(cls, document, stop_words=[],
        extra_stop_words=[], **kwargs):
        tokens = cls.tokenize(document)
        words = set(stop_words) or STOP_WORDS
        if extra_stop_words != []:
            words = words.union(set(extra_stop_words))
        document = " ".join(filter(lambda x: x not in words, tokens))
        return document

    stemmer = SnowballStemmer("portuguese")
    @classmethod
    def snowball_stemmer(cls, document, **kwargs):
        """Use nltk Snowball Stemmer to stemmize words"""
        tokens = cls.tokenize(document)
        document = ' '.join([cls.stemmer.stem(word) for word in tokens])
        return document
\end{lstlisting}

\end{apendicesenv}