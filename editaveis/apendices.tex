\begin{apendicesenv}

\partapendices
\chapter{Coleta de nomes dos dados abertos}
\label{sec:apendiceA}

Primeiramente, foi necessário baixar os dados no formato CSV para a máquina local. Em seguida, executar o código python.

\begin{lstlisting}[language=Python]
# Validator for CPF
def validate_cpf(cpf):
    cpf=str(cpf)
    if len(cpf) != 11:
        return False
    
    # CPF in form 00000000000 11111111111 ... 99999999999 
    # are invalid
    if cpf in [11 * str(i) for i in range(10)]:
        return False    
        
    # (10 * num1 + 9 * num2 + ... + 2 * num9) * 10 % 11
    # if rest == 10, result = 0
    zip_numbers = zip(range(10, 1, -1), cpf[:10])
    sumatory = [ x * int(i) for x, i in zip_numbers]
    result = ((sum(somatory) * 10) % 11) % 10
    if result != int(cpf[9]):
        return False
    
    # (11 * num1 + 10 * num2 + ... + 2 * num10) * 10 % 11
    # if rest == 10, result = 0
    zip_numbers = zip(range(11, 1, -1), cpf[:11])
    somatory = [ x * int(i) for x, i in zip_numbers]
    result = ((sum(somatory) * 10) % 11) % 10

    return result == int(cpf[10])
\end{lstlisting}

\begin{lstlisting}[language=Python]
import pandas as pd

dataFrame = pd.read_csv('201701_GastosDiretos.csv', 
	encoding='Latin-1', sep='\t')

# Create a new Column in data frame to define if the 'Codigo Favorecido' is a CPF
dataFrame['Codigo CPF Valido'] = dataFrame.progress_apply(
    lambda x: validate_cpf(x['Codigo Favorecido']),
    axis=1
)

# Filter data frame droping all unnecessary columns (axis = 1)
dataFrame = dataFrame[dataFrame['Codigo CPF Valido']]
dataFrame['name'] = dataFrame['Nome Favorecido']

# Generate a new data frame with names and surnames splited
names = dataFrame.name.str.cat(sep=' ').lower().split()
namesDF = pd.DataFrame(names, columns=['name'])
namesDF = namesDF.sort_values('name')

# Resize the data frame and add a new column of occurencies of the name 
namesDF = namesDF.groupby('name').size().reset_index(name='counts')
namesDF = namesDF[namesDF.name.str.len() > 2]

# Persist data to be used in other codes
namesDF.to_csv('names.csv', index=False)
\end{lstlisting}

\end{apendicesenv}

\chapter{Limpeza dos dados}
\label{sec:apendiceB}

O código abaixo foi utilizado para aplicar as expressões regulares, as normalizações e a transformação em símbolos.

\begin{lstlisting}[language=Python,extendedchars=true]
import re
from spacy.lang import pt
import nltk
from nltk.stem.snowball import SnowballStemmer

try:
    nltk.word_tokenize('some word')
except:
    nltk.download('punkt')

try:
    nltk.corpus.stopwords.words('portuguese')
except:
    nltk.download('stopwords')
finally:
    STOP_WORDS = pt.STOP_WORDS.union(
        set(nltk.corpus.stopwords.words('portuguese'))
    )


class CorpusHandler:

    def __init__(self):
        pass

    @staticmethod
    def clean_number(document, **kwargs):
        return re.sub(r'\s\d+\s', ' ', document)

    @staticmethod
    def clean_email(document, **kwargs):
        local_part = r"[0-9a-zA-Z!#$%&'*+-/=?^_`{|}~.]+"
        domain = r"[a-zA-Z][a-zA-Z0-9-.]*[a-zA-Z]\.\w{2,4}"
        document = re.sub(r"\s{}@{}".format(
                local_part, domain),
            ' EMAIL ', document)
        return document

    @staticmethod
    def clean_site(document, **kwargs):
        # Addaption for rules of
        # https://tools.ietf.org/html/rfc3986#section-3 to
        # stf's documents
        scheme = r"[a-zA-Z][a-zA-Z0-9+-.]*:?//?"
        host_ip = r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}'
        host_name = r"[a-zA-Z]+(\.[a-zA-Z0-9-_~]+)+"
        www = r'www\.{}'.format(host_name)
        scheme_host = r'({}{}|{}{}|{}{}|{})'.format(
            scheme, host_ip, scheme, www, scheme, host_name, www
        )
        port = r"(:\d+)?"
        resource = r"(/[a-zA-Z0-9-._~!$&'/*+,;=]*)?"
        query = r"(\?[a-zA-Z0-9-._~!$&'/*+,;=]*)?"
        fragment = r"(#[a-zA-Z0-9-._~!$&'/*+,;=]*)?"
        document = re.sub(r"{}{}{}{}{}".format(
                scheme_host, port, resource, query, fragment),
            ' SITE ', document)
        return document

    @staticmethod
    def transform_token(document, **kwargs):
        word_number = r'(n.?|numero|numero|n.?|no.?)?'
        # Law number: 00/YEAR or 00
        number_law = r'([0-9]+)((\s|\.)+\d+)?'
        matchs = [('LEI', 'lei'), ('ARTIGO', r'art\.|art\w*'),
                  ('DECRETO', 'decreto'),]

        for word, regex in matchs:
            document = re.sub(r'{}\s*{}\s*{}'.format(
                    regex, word_number, number_law),
                r'{}_\2'.format(word),
                document, flags=re.I)

        return document

    @staticmethod
    def remove_small_big_words(document, **kwargs):
        # remove 2 chars
        document = re.sub(r'\s\w{0,2}\s', ' ', document)
        # remove bigger words ex.: infrainconstitucionalidade
        document = re.sub(r'\s\w{30,}\s', ' ', document)
        return document

    @staticmethod
    def remove_letter_number(document, **kwargs):
        # Remove wor00 00wo 00wor00 wo00rd and keep WORD_000
        return re.sub(r'([A-Z]+_\d+)|[^ ]*\d+[^ ]*', r'\1', document)

    @staticmethod
    def clean_document(document, **kwargs):
        # Replace 0.0 for 00
        document = re.sub(r'(\d)\.(\d)', r'\1\2', document)

        # Remove all non alphanumeric
        document = re.sub(r'\W', ' ', document)

        return document

    @staticmethod
    def clean_spaces(document, **kwargs):
        # Remove multiple spaces
        document = re.sub(r'\s+', ' ', document)
        document = document.strip()
        return document

    @staticmethod
    def clean_alphachars(document, **kwargs):
        return re.sub(r'[^ ]*[^-_\'u\'i\'o\~o\^o\'e\^e\~a\'a\`a\^aa-z0-9\{\c c} ]+[^ ]*',
                    ' ', document)

    @staticmethod
    def tokenize(document, **kwargs):
        """Transform string documents in array of tokens"""
        if isinstance(document, str):
            document = document.split()
        return document

    @classmethod
    def remove_stop_words(cls, document, stop_words=[],
        extra_stop_words=[], **kwargs):
        tokens = cls.tokenize(document)
        words = set(stop_words) or STOP_WORDS
        if extra_stop_words != []:
            words = words.union(set(extra_stop_words))
        document = " ".join(filter(lambda x: x not in words, tokens))
        return document

    stemmer = SnowballStemmer("portuguese")
    @classmethod
    def snowball_stemmer(cls, document, **kwargs):
        """Use nltk Snowball Stemmer to stemmize words"""
        tokens = cls.tokenize(document)
        document = ' '.join([cls.stemmer.stem(word) for word in tokens])
        return document
\end{lstlisting}
